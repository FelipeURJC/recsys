{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "This notebook, by [felipe.alonso@bbva.com](mailto:felipe.alonso@bbva.com) \n",
    "(last version: v5. 02/02/2021)\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Gathering and analysing data](#load_data) \n",
    "2. [Most popular movies](#popular)\n",
    "3. [Collaborative Filtering](#cf)  \n",
    "4. [Latent Factor Models](#lfm)  \n",
    "5. [Exercises (advanced)](#exercises)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='load_data'></a>\n",
    "# 1. Load data\n",
    "\n",
    "We will use one the [MovieLens datasets](https://grouplens.org/datasets/movielens/), which consitutes one of the most common datasets for implementing and testing recommender engines. Specifically, we will be using the [Lastest Dataset](https://grouplens.org/datasets/movielens/latest/) (Small). This data set consists of:\n",
    "\n",
    "* **100836 ratings** across **9742 movies**. \n",
    "* Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n",
    "* These data were created by **610 users** between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "* Users were selected at random for inclusion. All selected **users had rated at least 20 movies**. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
    "* The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`. \n",
    "* Only movies with at least one rating or tag are included in the dataset. \n",
    "\n",
    "For further information read the [README file](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html).\n",
    "\n",
    "## 1.1 Load ratings\n",
    "\n",
    "<div class  = \"alert alert-info\"> \n",
    "Read the file <b>ratings.csv</b> and save it into a pandas Dataframe. Data is in the path './data/ml-latest-small/' \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/ml-latest-small/'\n",
    "\n",
    "# Hint: pandas read_csv function might be useful\n",
    "\n",
    "df = # YOUR CODE HERE\n",
    "\n",
    "# we add a new colum, year, containing the year of the timestamp\n",
    "df['year'] = df.timestamp.apply(lambda x: datetime.datetime.fromtimestamp(x).strftime('%Y'))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_users = df.userId.unique().shape[0]\n",
    "n_movies = df.movieId.unique().shape[0]\n",
    "print('#ratings =', df.shape[0])\n",
    "print('#users = ' + str(n_users) + '\\n#movies = ' + str(n_movies))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Ratings analytics\n",
    "\n",
    "Let's go deeper into our data, trying to answer questions like:\n",
    "\n",
    "1. Which is the average rating for all movies? \n",
    "2. And the distribution of ratings? \n",
    "3. And the average rating by time (years)?\n",
    "4. And the average number of rated movies per user? \n",
    "5. And the distribution of rated movies per user?\n",
    "\n",
    "<div class  = \"alert alert-info\"> \n",
    "<b> QUESTION </b>: Show (you can do some drawings if you want) some statistics to understand your data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's give some metrics\n",
    "\n",
    "# 1) Average rating for all times\n",
    "avg_rating = # YOUR CODE HERE\n",
    "print('Avg. rating :', avg_rating)\n",
    "\n",
    "# 2) Rating distribution\n",
    "#... YOUR CODE HERE\n",
    "plt.show()\n",
    "\n",
    "# 3) Average rating by year\n",
    "#... YOUR CODE HERE\n",
    "plt.show()\n",
    "\n",
    "# 4) Average number of rated movies per user\n",
    "#... YOUR CODE HERE\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(data_path + 'movies.csv',sep=',')\n",
    "\n",
    "print('Number of rows: ', movies.shape[0])\n",
    "print('Number of (unique) movies: ',len(movies.movieId.unique()))\n",
    "movies.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Duplicated movieId\n",
    "\n",
    "Unfortunately, the dataset is not completely clear. **There are a few movies with several `ids`**. Let's find them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_movies_per_id = movies[['movieId','title']].groupby('title').count().sort_values('movieId',ascending=False)\n",
    "n_movies_per_id.columns = ['n_ids']\n",
    "n_movies_per_id = n_movies_per_id[n_movies_per_id.n_ids>1]\n",
    "\n",
    "movieId_to_remove = []\n",
    "for t in n_movies_per_id.index.values:\n",
    "    print('The movie:',t,', has these ids: ', movies.loc[movies.title == t].movieId.values)\n",
    "    movieId_to_remove.append(movies.loc[movies.title == t].movieId.values[-1])\n",
    "\n",
    "print(\"\\nWe have to remove these indexes values: \", movieId_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we remove the `movies.title` duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.drop_duplicates(subset=['title'],keep='first')\n",
    "\n",
    "print(movies.shape)\n",
    "movies.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should remove also all entries in ratings data where `movieId == movieId_to_remov` (note that there would be other possibilities like changing the `movieId` in ratings data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df.movieId.isin(movieId_to_remove)]\n",
    "\n",
    "n_users = df.userId.unique().shape[0]\n",
    "n_items = df.movieId.unique().shape[0]\n",
    "print('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items)  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Not rated movies\n",
    "\n",
    "As you can see, there are **9722 rated movies** (`ratings.csv`), while there are **9737 distinct movies** in the database (`movies.csv`). Let's find the list of movies that have not been rated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratedMovieIds = df.movieId.unique()\n",
    "notRatedMoviesTitles = movies.loc[~movies.movieId.isin(ratedMovieIds),['movieId','title']]\n",
    "\n",
    "print('There are', len(notRatedMoviesTitles), 'movies with no rating, which are:\\n')\n",
    "print('movieId\\t title')\n",
    "print(notRatedMoviesTitles.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 reindexing users and movies\n",
    "\n",
    "\n",
    "#### Movies Ids\n",
    "\n",
    "Let's move forward. In order to analyze the predicted recommendations, let's create a python dictonary for translating `movieId` to movie titles, and `movieId` to an integer id (`idx`). This way `idx` will vary between 0 and 9137-1 (movies in `movies.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_id_list = np.unique(movies.movieId.values)\n",
    "\n",
    "# MovieId to idx\n",
    "movieId_to_idx = {movieId:idx for idx, movieId in enumerate(movies_id_list)}\n",
    "movies['movieIdx'] = movies['movieId'].apply(lambda x: movieId_to_idx[x])\n",
    "\n",
    "plt.scatter(movies.movieIdx,movies.movieId)\n",
    "plt.xlabel('Idx')\n",
    "plt.ylabel('MovieId')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See the result\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modify `ratings` to include new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_title = {idx:title for idx,title in zip(movies.movieIdx.values,movies.title.values)}\n",
    "\n",
    "df['movieIdx'] = df['movieId'].apply(lambda x: movieId_to_idx[x])\n",
    "df['title'] = df['movieIdx'].apply(lambda x: idx_to_title[x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, `year` does not represent the year of release. Let's calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_year(title):\n",
    "    \n",
    "    p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
    "    m = p.search(title)\n",
    "    year = m.group(1)\n",
    "    \n",
    "    return year\n",
    "\n",
    "df['year_release'] = df.title.apply(lambda x:extract_year(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Ids\n",
    "\n",
    "User ids start from 1 to 610, and we will change it to range between 0 a 609 (they are assigned sequentially)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.userId = df.userId -1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve Movies IDs\n",
    "\n",
    "Define a function that retrieves all the ids and titles for movies containing 'text' in its title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_movie_id(text, ids):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - text: string to be looked for in movies titles\n",
    "    - ids: dictionary of {id:title}\n",
    "    \n",
    "    Return: \n",
    "    - A list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [(k, v) for k, v in list(ids.items()) if v.lower().find(text.lower()) > -1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTitle = 'Star Wars'\n",
    "return_movie_id(queryTitle, idx_to_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "<b>QUESTION</b>: in the previous cell, change <tt>queryTitle</tt> to find the index of a movie of your taste. Change also <tt>idx_to_movieTitle</tt> for <tt>movieId_to_movieTitle</tt> and check out the differences.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='popular'></a>\n",
    "# 2. Most popular movies\n",
    "\n",
    "Movies can be ranked according to different popularity metrics:\n",
    "* Most rated movie (it is assumed that this is the most watched movie)\n",
    "* Highest averaged rated movie\n",
    "\n",
    "## 2.1 Most rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_rated_movie = df[['userId','title']].groupby(['title']).count().sort_values('userId',ascending=False)\n",
    "most_rated_movie.columns = ['popularity_score']\n",
    "\n",
    "most_rated_movie.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Highest averaged rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating_per_movie = df[['title','rating']].groupby('title').agg(['mean', 'count'])\n",
    "avg_rating_per_movie.columns = ['avg_rating','n_raters']\n",
    "avg_rating_per_movie.head()\n",
    "\n",
    "min_raters = 1\n",
    "avg_rating_per_movie = avg_rating_per_movie[avg_rating_per_movie.n_raters>=min_raters]\n",
    "avg_rating_per_movie = avg_rating_per_movie.sort_values(['avg_rating'],ascending=False)\n",
    "\n",
    "avg_rating_per_movie.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "<b> QUESTION </b>: Change the value of <tt>min_raters</tt> and re-run the above cell. What happens now? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='cf'></a>\n",
    "# 3. Collaborative filtering\n",
    "\n",
    "## 3.1 Naïve approach\n",
    "\n",
    "*\"People who watched (rated/purchased) this movie (product) also watched (rated/purchased)...\"* \n",
    "\n",
    "Let's build our co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rated movies:', len(df.movieIdx.unique()))\n",
    "print('Number of movies:', len(movies.movieIdx.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "<b> QUESTION </b>: Aiming to build a co-occurrence matrix, which should be the dimensions of this matrix? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Build co-occurence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might be useful\n",
    "movies_per_user_dict = {user_id:idx.values for user_id,idx in df.groupby('userId')['movieIdx']}\n",
    "\n",
    "# check movies_per_user_dict (uncomment the following lines if necessary)\n",
    "\n",
    "#this_user_id = 0\n",
    "#movies_per_user_dict[this_user_id]\n",
    "#[idx_to_title[k] for k in movies_per_user_dict[this_user_id]] # retrieve titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = # YOUR CODE HERE \n",
    "print(N)\n",
    "\n",
    "coMatrix = np.zeros((N, N)) # co-occurrence matrix\n",
    "for userId, ids in movies_per_user_dict.items():\n",
    "    for m in ids:\n",
    "        coMatrix[m,ids] = # YOUR CODE HERE\n",
    "\n",
    "print(coMatrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coMatrix[0:10,0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Predictions using the co-occurrence matrix\n",
    "\n",
    "Now, let's make predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTitle = 'lambs'\n",
    "return_movie_id(queryTitle, idx_to_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMovieId = 510 # YOUR CODE HERE\n",
    "print('Let\\'s make recommendations for: ', idx_to_title[queryMovieId])\n",
    "print('')\n",
    "\n",
    "# Get the corresponding row \n",
    "queryAnswer = # YOUR CODE HERE\n",
    "\n",
    "# Get the highest counts (np.argsort might be useful, in descending order)\n",
    "queryAnswer = # YOUR CODE HERE\n",
    "\n",
    "# Get the highest counts (do not select the query movie)\n",
    "queryAnswer = # YOUR CODE HERE\n",
    "\n",
    "# let's print out the first 20 recommendations\n",
    "printAnswer = queryAnswer[0:20]\n",
    "for i,answerId in enumerate(printAnswer):\n",
    "    print(i+1,'-', idx_to_title[answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "<b> QUESTION </b>: How accurate are these recommendations? Alternatives?\n",
    "</div>\n",
    "\n",
    "### 3.1.3 Jaccard similarity \n",
    "\n",
    "What about using the [Jaccard similarity index](https://en.wikipedia.org/wiki/Jaccard_index)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = queryMovieId\n",
    "jaccardScore = np.zeros(N-1)\n",
    "for j in range(N-1):\n",
    "    num = # YOUR CODE HERE \n",
    "    dem = # YOUR CODE HERE \n",
    "    jaccardScore[j] = float(num)/float(dem) \n",
    "\n",
    "queryAnswer = np.argsort(jaccardScore)[::-1] #descending order\n",
    "queryAnswer = queryAnswer[1:] \n",
    "\n",
    "# let's print out the first 20 recommendations\n",
    "printAnswer = queryAnswer[0:20]\n",
    "for i,answerId in enumerate(printAnswer):\n",
    "    print(i+1,'-', idx_to_title[answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Memory-Based Collaborative Filtering (CF)\n",
    "\n",
    "Memory-Based Collaborative Filtering approaches can be divided into two main sections: **user-user filtering** and **item-item filtering**. \n",
    "\n",
    "* User-user CF: *“Users who are similar to you also liked …”*. A *user-user filtering* will take a particular user, find users that are similar to that user based on similarity of ratings, and recommend items that those similar users liked. \n",
    "\n",
    "* Item-Item CF: *“Users who liked this movie also liked …”*. In contrast, *item-item filtering* will take an item, find users who liked that item, and find other items that those users or similar users also liked. \n",
    "\n",
    "\n",
    "First, we need to build our utility matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just as a reminder of the information\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build utility matrix\n",
    "n_items = # YOUR CODE HERE \n",
    "n_users = # YOUR CODE HERE \n",
    "print(n_users, 'x', n_items)\n",
    "\n",
    "uMatrix = np.zeros((n_users, n_items)) # utility matrix\n",
    "\n",
    "for row in df.itertuples():\n",
    "    user_id = row[1]\n",
    "    item_id = row[6]\n",
    "    uMatrix[user_id, item_id] = # YOUR CODE HERE \n",
    "\n",
    "print('Dimensions: ', uMatrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do some checking ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show uMatrix\n",
    "print(uMatrix[0:10,0:10])\n",
    "\n",
    "# check for an specific user\n",
    "print(movies_per_user_dict[5][0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity\n",
    "\n",
    "Now, we define our similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # epsilon -> small number for handling dived-by-zero errors\n",
    "    if kind == 'user':\n",
    "        sim = ratings.dot(ratings.T) + epsilon\n",
    "    elif kind == 'item':\n",
    "        sim = ratings.T.dot(ratings) + epsilon\n",
    "    norms = np.array([np.sqrt(np.diagonal(sim))])\n",
    "    return (sim / norms / norms.T)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now calculate our similarity matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userSimilarity = cosineSimilarity(uMatrix, kind='user')\n",
    "print('user-user: ', userSimilarity.shape)\n",
    "\n",
    "itemSimilarity = cosineSimilarity(uMatrix, kind='item')\n",
    "print('item-item: ', itemSimilarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some checking\n",
    "print(userSimilarity[0:12,0:12].round(2))\n",
    "print('')\n",
    "print(itemSimilarity[0:12,0:12].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell\n",
    "def findKsimilars(rowId,simMatrix,k=5):\n",
    "    qAnswer = simMatrix[rowId,:]\n",
    "    qIds = np.argsort(qAnswer)[::-1]\n",
    "    qValues = simMatrix[rowId,qIds]\n",
    "    \n",
    "    return [(qIds[j],qValues[j]) for j in range(1,k+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryUser = 0\n",
    "moreSimilarUsers = findKsimilars(queryUser,userSimilarity,k=5)\n",
    "\n",
    "print('The more similar users to USER_ID =', queryUser, 'are:\\n')\n",
    "\n",
    "for u,v in moreSimilarUsers:\n",
    "    print('User',u, 'with a similarity of ', v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find similar items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryTitle = 'mallrats'\n",
    "return_movie_id(queryTitle, idx_to_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMovieId = 510\n",
    "moreSimilarItems = findKsimilars(queryMovieId,itemSimilarity,k=5)\n",
    "\n",
    "print('The more similar movies to ', idx_to_title[queryMovieId], 'are:\\n')\n",
    "\n",
    "for item in moreSimilarItems:\n",
    "    print(idx_to_title[item[0]], 'with a similarity of ', item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "<b>Exercise (advanced - optional):</b> Implement centered cosine similarity metric\n",
    "</div>\n",
    "\n",
    "```python\n",
    "def centeredCosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    # YOUR CODE HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Predictions for specific user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useruserCFpredictions = userSimilarity.dot(uMatrix) / np.array([np.abs(userSimilarity).sum(axis=1)]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryUser = 500\n",
    "\n",
    "queryAnswer = uMatrix[queryUser,:]\n",
    "queryAnswer = np.argsort(queryAnswer)[::-1] #descending order\n",
    "\n",
    "print('User_ID: ' + str(queryUser) + ', liked the most:')\n",
    "print(' ')\n",
    "# let's print out the first 20 recommendations\n",
    "printAnswer = queryAnswer[0:11]\n",
    "for answerId in printAnswer:\n",
    "    print(uMatrix[queryUser,answerId],'-',idx_to_title[answerId])\n",
    "    \n",
    "print('\\n... and he/she has not seen ... ')\n",
    "\n",
    "noWatchedMovies = np.where(uMatrix[queryUser,:]==0)[0]\n",
    "for m in noWatchedMovies[0:11]:\n",
    "    print(idx_to_title[m])\n",
    "    \n",
    "print('\\n... among others ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAnswer = useruserCFpredictions[queryUser,noWatchedMovies]\n",
    "queryAnswer = noWatchedMovies[np.argsort(queryAnswer)[::-1]] #descending order\n",
    "\n",
    "print('so, it is expected he/she also likes ... ')\n",
    "print(' ')\n",
    "\n",
    "printAnswer = queryAnswer[0:11]\n",
    "for answerId in printAnswer:\n",
    "    print(idx_to_title[answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. CF Evaluation\n",
    "\n",
    "We need a training and a test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainData, testData = train_test_split(df, test_size=0.25, random_state=0)\n",
    "\n",
    "print('Training data: ', trainData.shape)\n",
    "print('Test data: ', testData.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTraining = np.zeros((n_users, n_items)) # utility matrix\n",
    "for row in trainData.itertuples():\n",
    "    uMatrixTraining[row[1], row[6]] = row[3]\n",
    "    \n",
    "uMatrixTest = np.zeros((n_users, n_items)) # utility matrix\n",
    "for row in testData.itertuples():\n",
    "    idx = movieId_to_idx[row[2]]\n",
    "    uMatrixTest[row[1], row[6]] = row[3]\n",
    "\n",
    "print(uMatrixTraining.shape)\n",
    "print(uMatrixTest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cosine similarity\n",
    "userSimilarity = cosineSimilarity(uMatrixTraining, kind='user')\n",
    "print(userSimilarity.shape)\n",
    "\n",
    "useruserCFpredictions = userSimilarity.dot(uMatrixTraining) / np.array([np.abs(userSimilarity).sum(axis=1)]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def rmse(prediction, ground_truth):\n",
    "    prediction = prediction[ground_truth.nonzero()].flatten() \n",
    "    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n",
    "    return np.sqrt(mean_squared_error(prediction, ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's evaluate\n",
    "print('User-based CF RMSE: ' + str(rmse(useruserCFpredictions, uMatrixTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanUserRating = uMatrixTraining.mean(axis=1)\n",
    "\n",
    "ratingsDiff = (uMatrixTraining - meanUserRating[:, np.newaxis]) \n",
    "userItemCFGlobalpredictions = meanUserRating[:, np.newaxis] + userSimilarity.dot(ratingsDiff) / np.array([np.abs(userSimilarity).sum(axis=1)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF global baseline RMSE: ' + str(rmse(userItemCFGlobalpredictions, uMatrixTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### item-item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemItemCFpredictions = uMatrixTraining.dot(itemSimilarity) / np.array([np.abs(itemSimilarity).sum(axis=1)]) \n",
    "\n",
    "print('item-based CF RMSE: ' + str(rmse(itemItemCFpredictions, uMatrixTest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='lfm'></a>\n",
    "## 4. Model-based CF or Latent factor models\n",
    "\n",
    "### 4.1 Singular value decomposition\n",
    "\n",
    "CF can be formulated in terms of latent factors by approximating the utility matrix using singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "#get SVD components from train matrix. Choose k.\n",
    "u, s, vt = svds(uMatrixTraining, k = 20)\n",
    "s_diag_matrix=np.diag(s)\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('User-based CF MSE: ' + str(rmse(svdPredictions, uMatrixTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAnswer = svdPredictions[queryUser,noWatchedMovies]\n",
    "queryAnswer = noWatchedMovies[np.argsort(queryAnswer)[::-1]] #descending order\n",
    "\n",
    "print('so, it is expected he/she also likes ... ')\n",
    "print(' ')\n",
    "\n",
    "printAnswer = queryAnswer[0:11]\n",
    "for answerId in printAnswer:\n",
    "    print(idx_to_title[answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Alternating Least Squares (ALS)\n",
    "\n",
    "SVD can be very slow and computationally expensive. Besides, when addressing only the relatively few known entries we are highly prone to overfitting.\n",
    "\n",
    "An scalable alternative to SVD is ALS, which can include regularization terms to prevent overfitting. We will rename our variable to make them more similar to the ALS notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = uMatrixTraining\n",
    "T = uMatrixTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a “selector” matrix $I$ for the training utility matrix $R$, which will contain 0 if the rating matrix has no rating entry, and 1 if the rating matrix contains an entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "I = R.copy()\n",
    "I[I > 0] = 1\n",
    "I[I == 0] = 0\n",
    "\n",
    "# Index matrix for test data\n",
    "I2 = T.copy()\n",
    "I2[I2 > 0] = 1\n",
    "I2[I2 == 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "The ALS algorithm aims to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alsRmse(I,R,Q,P):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P.T,Q)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm free parameters\n",
    "lmbda = 0.1     # Regularisation weight\n",
    "k = 20          # Dimensionality of latent feature space\n",
    "m, n = R.shape  # Number of users and items\n",
    "n_epochs = 5   # Number of epochs\n",
    "\n",
    "# Initialization\n",
    "P = 3 * np.random.rand(k,m) # Latent user feature matrix\n",
    "Q = 3 * np.random.rand(k,n) # Latent movie feature matrix\n",
    "Q[0,:] = R[R != 0].mean(axis=0) # Avg. rating for each movie\n",
    "E = np.eye(k) # (k x k)-dimensional idendity matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes a while ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "# Repeat until convergence\n",
    "for epoch in range(n_epochs):\n",
    "    # Fix Q and estimate P\n",
    "    for i, Ii in enumerate(I):\n",
    "        nui = np.count_nonzero(Ii) # Number of items user i has rated\n",
    "        if (nui == 0): nui = 1 # Be aware of zero counts!\n",
    "    \n",
    "        # Least squares solution\n",
    "        Ai = np.dot(Q, np.dot(np.diag(Ii), Q.T)) + lmbda * nui * E\n",
    "        Vi = np.dot(Q, np.dot(np.diag(Ii), R[i].T))\n",
    "        P[:,i] = np.linalg.solve(Ai,Vi)\n",
    "        \n",
    "    # Fix P and estimate Q\n",
    "    for j, Ij in enumerate(I.T):\n",
    "        nmj = np.count_nonzero(Ij) # Number of users that rated item j\n",
    "        if (nmj == 0): nmj = 1 # Be aware of zero counts!\n",
    "        \n",
    "        # Least squares solution\n",
    "        Aj = np.dot(P, np.dot(np.diag(Ij), P.T)) + lmbda * nmj * E\n",
    "        Vj = np.dot(P, np.dot(np.diag(Ij), R[:,j]))\n",
    "        Q[:,j] = np.linalg.solve(Aj,Vj)\n",
    "    \n",
    "    train_rmse = alsRmse(I,R,Q,P)\n",
    "    test_rmse = alsRmse(I2,T,Q,P)\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    \n",
    "    print(\"[Epoch %d/%d] train error: %f, test error: %f\"%(epoch+1, n_epochs, train_rmse, test_rmse))\n",
    "    \n",
    "print(\"Algorithm converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "plt.title('ALS-WR Learning Curve')\n",
    "plt.xlabel('Number of Epochs');\n",
    "plt.ylabel('RMSE');\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsPredictions = np.dot(P.T,Q)\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "\n",
    "print('SVD RMSE: ' + str(rmse(svdPredictions, uMatrixTest)))\n",
    "print('ALS RMSE: ' + str(rmse(alsPredictions, uMatrixTest)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryAnswer = alsPredictions[queryUser,noWatchedMovies]\n",
    "queryAnswer = noWatchedMovies[np.argsort(queryAnswer)[::-1]] #descending order\n",
    "\n",
    "print('so, it is expected he/she also likes ... ')\n",
    "print(' ')\n",
    "\n",
    "printAnswer = queryAnswer[0:11]\n",
    "for answerId in printAnswer:\n",
    "    print(idx_to_title[answerId])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='exercises'></a>\n",
    "## 5. Exercises (advanced)\n",
    "\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "<b>E1:</b> Implement centered cosine similarity metric in Section 3.2\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "<b>E2:</b> Implement global baseline biased in 3.2: $b_{ui} = \\mu + b_u + b_i$\n",
    "</div>\n",
    "\n",
    "<div class = \"alert alert-success\">\n",
    "<b>E3:</b> Implement k-neighbors in 3.2\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
